{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:Maroon\">Case Study: Usage of Data Science in Entertainment Industry.\n",
    "\n",
    "#### Select optimal advetisement to display in break time for a TV Show, based on what was recently shown in the TV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:Maroon\">Explanation of Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Maroon\">Performance Metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Maroon\">Accuracy\n",
    "\n",
    "Accuracy is equal to the number of observations correctly classified over all observations. For example, if the model properly identified 77 out of 100 samples, we have an accuracy of 77%. Mathematically, it is simply\n",
    "\n",
    "$$ \\frac{\\text{number of correct observations}}{\\text{number of observations}}.$$\n",
    "\n",
    "However, accuracy may not always be a good metric. Consider the case of problem with only 10% positive cases. A naive classifier that always predicts the majority class will achieve 90% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Maroon\">Precision and recall\n",
    "\n",
    "In case, we were more interested in determining our model's performance with regards to the class '1', we need to look at precision and recall. In our case, '1' is positive class and '0' is negative class.\n",
    "\n",
    "Precision is the fraction of true positives over all positive predictions. It is a measure of how \"precise\" our model was with regards to labeling observations as positive. \n",
    "\n",
    "Recall, on the other hand, is equal to the fraction of true positives over all positive observations. It is a measure of our model's ability to \"catch\" and properly label observations that are positive.\n",
    "\n",
    "A confusion matrix is a table summarizing the performance of the model by enumerating true and false positives and the true and false negatives.\n",
    "\n",
    "|                     | Positive Observation     | Negative Observation    |\n",
    "|---------------------|:------------------------:|:-----------------------:|\n",
    "| Positive Prediction |     True Positive (TP)   | False Positive (FP)     |\n",
    "| Negative Prediction | False Negative (FN)      |     True Negative (TN)  |\n",
    "\n",
    "The equation for precision and recall are\n",
    "\n",
    "$$ \\text{precision} = \\frac{\\text{TP}}{TP + FP}$$\n",
    "and\n",
    "$$ \\text{recall} = \\frac{\\text{TP}}{TP + FN}. $$\n",
    "\n",
    "For a naive model that predicts the majority class, the recall would be 0 and our precision would be undefined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Maroon\">F1-Score\n",
    "The F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score:\n",
    "\n",
    "$$ \\text{f1-score} = \\frac{\\text{2*precision*recall}}{precision + recall}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:Maroon\">Comments on Performance Metrics:\n",
    "F1 Score gives equal importance to precision and recall. In practice, different types of mis-classifications incur different costs. Or the relative importance of precision and recall is an aspect of the problem.\n",
    "\n",
    "Few other performance metrics are:\n",
    "    1. Matthews correlation coefficient\n",
    "    2. Cohen's kappa\n",
    "    \n",
    "    \n",
    "For our problem, we will focus on selecting a classifier high accuracy and f1-score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
